{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 22:13:15.273077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-08 22:13:15.429097: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-08 22:13:15.458984: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-08 22:13:16.028739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: ::/usr/local/cuda/lib64\n",
      "2022-09-08 22:13:16.028858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: ::/usr/local/cuda/lib64\n",
      "2022-09-08 22:13:16.028866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "train, valiadation,test's shape: 140 500 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/graphgallery/data/preprocess.py:246: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(nx.from_dict_of_lists(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, valiadation,test's batch num: 3 8 16\n",
      "data: cora, num_node_classes: 7\n",
      "testset'acc: device=cuda:0, dataset=cora, batch_size=64, learning_rate=0.002, T=200, max_test_accuracy=0.8080, num_s1: 17683, num_s2: 196, num_s_per_node: 17879rain_accuracy=1.0000,max_val_accuracy=0.7900, train_times=600\n",
      "acc_averages 0001 times: means: 0.808000 std: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import datareader, sharedutils, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data as Data\n",
    "from model_lif_fc_with_val import model_lif_fc\n",
    "\n",
    "def snn_run(dataname, **new_conf):\n",
    "  conf, cnf = sharedutils.read_config(), {}\n",
    "  cnf.update(conf['shared_conf'])\n",
    "  cnf.update(conf['snn'][dataname])\n",
    "  # may be some new params\n",
    "  cnf.update(new_conf)\n",
    "  cnf['log_dir'] = conf['snn']['log_dir']\n",
    "  if cnf['v_reset'] == -100: cnf['v_reset'] = None\n",
    "  print(\"batch_size:\", cnf[\"batch_size\"])\n",
    "  rd = datareader.ReadData(\"~/datasets/datafromgg\")\n",
    "\n",
    "  # get fixed datasetï¼Œfixed split\n",
    "  data_fixed = rd.get_fixed_splited_data(dataname)\n",
    "#   data_fixed = rd.read_raw_data(dataname)  \n",
    "  data =data_fixed\n",
    "  mat, tag = rd.conv_graph(data_fixed)\n",
    "  if dataname==\"pubmed\" : mat = mat+0.05\n",
    "  if dataname==\"citeseer\" : mat = mat+0.05\n",
    "  if dataname==\"cora\" : mat = mat+0.05  \n",
    "  tr_ind, val_ind, ts_ind = data.split_nodes().train_nodes, \\\n",
    "  data.split_nodes().val_nodes, data.split_nodes().test_nodes\n",
    "\n",
    "        \n",
    "  print(\"train, valiadation,test's shape:\", len(tr_ind), len(val_ind), len(ts_ind))\n",
    "  tr_val_ind = np.hstack((tr_ind,val_ind))\n",
    "     \n",
    "\n",
    "  tr_val_mat = mat[tr_val_ind]\n",
    "  tr_val_tag = tag[tr_val_ind]\n",
    "  tr_mat = mat[tr_ind]\n",
    "  tr_tag = tag[tr_ind]\n",
    "  val_mat = mat[val_ind]\n",
    "  val_tag = tag[val_ind]\n",
    "  ts_mat = mat[ts_ind]\n",
    "  ts_tag = tag[ts_ind]\n",
    "  k = pd.DataFrame(mat)\n",
    "  u = k.describe()\n",
    "\n",
    "\n",
    "  self_sample = False\n",
    "  \n",
    "  if self_sample==True: \n",
    "#     print(\"tr_mat.shape()\",u)\n",
    "    train_data_loader, val_data_loader, test_data_loader = rd.sample_numpy2dataloader(20,data_fixed, mat, tag, batch_size=cnf[\"batch_size\"])\n",
    "  else: \n",
    "#     print(\"tr_mat.shape()\",u)\n",
    "    train_data_loader, val_data_loader, test_data_loader = rd.tr_ts_val_numpy2dataloader(tr_mat, ts_mat, val_mat, tr_tag,\n",
    "      ts_tag, val_tag, batch_size=cnf[\"batch_size\"])\n",
    "    \n",
    "       \n",
    "\n",
    "  \n",
    "  print(\"train, valiadation,test's batch num:\", len(train_data_loader), len(val_data_loader), len(test_data_loader))\n",
    "  \n",
    "  n_nodes, n_feat, n_flat = mat.shape[0], mat.shape[1], 1\n",
    "  print(\"data: %s, num_node_classes: %d\" % (dataname, data.graph.num_classes))\n",
    "#   print(cnf)\n",
    "  ret = model_lif_fc(device=cnf[\"device\"], dataset_dir=cnf[\"dataset_dir\"],\n",
    "                     dataname=dataname, batch_size=cnf[\"batch_size\"], \n",
    "                     learning_rate=cnf[\"learning_rate\"], T=cnf[\"T\"], tau=cnf[\"tau\"], \n",
    "                     v_reset=cnf[\"v_reset\"], v_threshold=cnf[\"v_threshold\"],\n",
    "                     train_epoch=cnf[\"train_epoch\"], log_dir=cnf[\"log_dir\"], n_labels=data.graph.num_classes,\n",
    "                     n_dim0=n_nodes, n_dim1=n_flat, n_dim2=n_feat, train_data_loader=train_data_loader,\n",
    "                     val_data_loader=val_data_loader, test_data_loader=test_data_loader)\n",
    "  \n",
    "  return ret\n",
    "\n",
    "def model_startup(dataname, runs, **new_conf):\n",
    "  scores = []\n",
    "  submits = []\n",
    "\n",
    "  conc = False\n",
    "  if conc:\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    pool = ThreadPoolExecutor(10)\n",
    "    for run in range(runs):\n",
    "      obj = pool.submit(snn_run, dataname, **new_conf)\n",
    "      submits.append(obj)\n",
    "    pool.shutdown(wait=True)\n",
    "    for sub in submits:\n",
    "      scores.append(sub.result())\n",
    "  else:\n",
    "    for run in range(runs):\n",
    "      score = snn_run(dataname, **new_conf)\n",
    "      scores.append(score)\n",
    "  sharedutils.add_log(os.path.join(\"./tmpdir/snn/\", \"snn_search.log\"), \"-1.5: \" + str(scores))\n",
    "  return np.mean(scores), np.std(scores)\n",
    "\n",
    "def search_params(dataname, runs, log_dir):\n",
    "  params_set = {\"learning_rate\": np.array([0.01, 0.015, 0.02, 0.025, 0.03]),\n",
    "                \"T\": np.array([200, 300, 400, 500]),\n",
    "                # \"tau\": np.array([80, 100, 120]),\n",
    "                # \"v_reset\": np.array([0.0, -1.0]),\n",
    "                # \"v_threshold\": np.array([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "                }\n",
    "\n",
    "  best_score, std, best_params = sharedutils.grid_search(dataname, runs, params_set, model_startup)\n",
    "  msg = \"sgc; %s; best_score, std, best_params %s %s %s\\n\" % (dataname, best_score, std, best_params)\n",
    "  print(msg)\n",
    "  sharedutils.add_log(os.path.join(log_dir, \"snn_search.log\"), msg)\n",
    "\n",
    "dataname = \"cora\"\n",
    "runs = 1\n",
    "me, st = model_startup(dataname, runs)\n",
    "print(\"acc_averages %04d times: means: %04f std: %04f\" % (runs, me, st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphgallery as gg\n",
    "from graphgallery import functional as gf\n",
    "from graphgallery.datasets import Planetoid, NPZDataset\n",
    "data = Planetoid('cora', root=\"~/datasets/datafromgg\", verbose=False)\n",
    "graph = data.graph\n",
    "splits = data.split_nodes()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
